{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vlCGBfBEF_o2"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "# import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CsF869K6GvDQ"
      },
      "outputs": [],
      "source": [
        "# tf.__version__\n",
        "# device_name = tf.test.gpu_device_name()\n",
        "# if device_name != '/device:GPU:0':\n",
        "#   raise SystemError('GPU device not found')\n",
        "# print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Bwq4ANzaGTw2"
      },
      "outputs": [],
      "source": [
        "# The x tuple contains the images and the y tuple contains the ground truths of these images\n",
        "x, y = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WmVAlzZcGdSY"
      },
      "outputs": [],
      "source": [
        "#Define input image dimensions\n",
        "#Large images take too much time and resources.\n",
        "img_rows = 28\n",
        "img_cols = 28\n",
        "channels = 1\n",
        "img_shape = (img_rows, img_cols, channels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zbiXu0g2Gr3N"
      },
      "outputs": [],
      "source": [
        "#Given input of noise (latent) vector, the Generator produces an image.\n",
        "def build_generator():\n",
        "\n",
        "    noise_shape = (100,) #1D array of size 100 (latent vector / noise)\n",
        "\n",
        "    #Define your generator network \n",
        "    #Here we are only using Dense layers. But network can be complicated based\n",
        "    #on the application. For example, you can use VGG for super res. GAN.         \n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(256, input_shape=noise_shape))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(512))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(1024))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    \n",
        "    # Can use sigmoid maybe\n",
        "    model.add(Dense(np.prod(img_shape), activation='tanh'))\n",
        "    model.add(Reshape(img_shape))\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    noise = Input(shape=noise_shape)\n",
        "    generated_img = model(noise)    #Generated image\n",
        "\n",
        "    return Model(noise, generated_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7-p92ybiKCBE"
      },
      "outputs": [],
      "source": [
        "#Given an input image, the Discriminator outputs the likelihood of the image being real.\n",
        "#Binary classification - true or false (we're calling it score)\n",
        "\n",
        "def build_discriminator():\n",
        "\n",
        "    model = Sequential([\n",
        "        Flatten(input_shape=img_shape),\n",
        "        Dense(512),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "        Dense(256),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.summary()\n",
        "    img = Input(shape=img_shape)\n",
        "    score = model(img)\n",
        "    return Model(img, score)\n",
        "\n",
        "#The score is the Discriminator’s guess of input being real or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZJse7ttVLBpJ"
      },
      "outputs": [],
      "source": [
        "#Now that we have constructed our two models it’s time to pit them against each other.\n",
        "#We do this by defining a training function, loading the data set, re-scaling our training\n",
        "#images and setting the ground truths.\n",
        "\n",
        "def train(epochs, batch_size=128, save_interval=100):\n",
        "\n",
        "    # Load the (real images) dataset\n",
        "    (X_train, _), (_, _) = mnist.load_data()\n",
        "\n",
        "    # Convert to float and Rescale -1 to 1 (Can also do 0 to 1)\n",
        "    X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
        "\n",
        "    #Add channels dimension. As the input to our gen and discr. has a shape 28x28x1.\n",
        "    X_train = np.expand_dims(X_train, axis=3) \n",
        "\n",
        "    # Define the number of fake images that will be fed to the discriminator at a time\n",
        "    half_batch = int(batch_size / 2)\n",
        "\n",
        "    #We then loop through a number of epochs to train our Discriminator by first selecting\n",
        "    #a random batch of images from our true dataset, generating a set of images from our\n",
        "    #Generator, feeding both set of images into our Discriminator, and finally setting the\n",
        "    #loss parameters for both the real and fake images, as well as the combined loss. \n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "        # Select a random half batch of real images from the 60,000 images in the dataset\n",
        "        idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
        "        imgs = X_train[idx]\n",
        "        \n",
        "        # Generate normal noise / latent vector\n",
        "        noise = np.random.normal(0, 1, (half_batch, 100))\n",
        "\n",
        "        # Generate a half batch of fake images\n",
        "        gen_imgs = generator.predict(noise)\n",
        "\n",
        "        # Train the discriminator on real and fake images, separately\n",
        "        # Research showed that separate training is more effective. \n",
        "        # The np.ones and np.zeros show the class of the real and fake images respectively\n",
        "        d_loss_real = discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\n",
        "        d_loss_fake = discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n",
        "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "        # And within the same loop we train our Generator, by setting the input noise and\n",
        "        # ultimately training the Generator to have the Discriminator label its samples as valid\n",
        "        # by specifying the gradient loss.\n",
        "        # ---------------------\n",
        "        #  Train Generator\n",
        "        # ---------------------\n",
        "        #Create noise vectors as input for generator. \n",
        "        #Create as many noise vectors as defined by the batch size. \n",
        "        #Based on normal distribution. Output will be of size (batch size, 100)\n",
        "        noise = np.random.normal(0, 1, (batch_size, 100))\n",
        "\n",
        "        # The generator wants the discriminator to label the generated samples\n",
        "        # as valid (ones)\n",
        "        #This is where the generator is trying to trick discriminator into believing\n",
        "        #the generated image is true (hence value of y is 1)\n",
        "        valid_y = np.array([1] * batch_size)\n",
        "\n",
        "        # Generator is part of combined where it got directly linked with the discriminator\n",
        "        # Train the generator with noise as x and y as 1. \n",
        "        # Again, 1 as the output as it is adversarial and if generator did a great\n",
        "        #job of fooling the discriminator then the output would be 1 (true)\n",
        "        g_loss = combined.train_on_batch(noise, valid_y)\n",
        "\n",
        "\n",
        "        print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
        "\n",
        "        # If at save interval => save generated image samples\n",
        "        if epoch % save_interval == 0:\n",
        "            save_imgs(epoch)\n",
        "\n",
        "        #when the specific sample_interval is hit, we call the\n",
        "        #sample_image function. Which looks as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zsrP6M_yN06T"
      },
      "outputs": [],
      "source": [
        "def save_imgs(epoch):\n",
        "    r, c = 5, 5\n",
        "    noise = np.random.normal(0, 1, (r * c, 100))\n",
        "    gen_imgs = generator.predict(noise)\n",
        "\n",
        "    # Rescale images 0 - 1\n",
        "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "    fig, axs = plt.subplots(r, c)\n",
        "    cnt = 0\n",
        "    for i in range(r):\n",
        "        for j in range(c):\n",
        "            axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
        "            axs[i,j].axis('off')\n",
        "            cnt += 1\n",
        "    fig.savefig(\"/content/drive/MyDrive/Digits/Generated Images/mnist_%d.png\" % epoch)\n",
        "    plt.close()\n",
        "# This function saves our images for us to view"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Cb1QMAQrRnLm"
      },
      "outputs": [],
      "source": [
        "#Let us also define our optimizer for easy use later on.\n",
        "#That way if you change your mind, you can change it easily here\n",
        "optimizer = Adam(0.0002, 0.5)  #Learning rate and momentum."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQeS-IpaRyQK",
        "outputId": "37fd0f73-62cd-4f91-8173-fbc3997fe2df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 533,505\n",
            "Trainable params: 533,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "discriminator = build_discriminator();\n",
        "discriminator.compile(loss='binary_crossentropy',\n",
        "                      optimizer = optimizer,\n",
        "                      metrics = ['accuracy']);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haLv1gbiST3K",
        "outputId": "697e6b6a-d25d-43dc-f75d-ca1a23dfa654"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 256)               25856     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1024)              525312    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 784)               803600    \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 28, 28, 1)         0         \n",
            "=================================================================\n",
            "Total params: 1,493,520\n",
            "Trainable params: 1,489,936\n",
            "Non-trainable params: 3,584\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "generator = build_generator();\n",
        "generator.compile(loss = 'binary_crossentropy', optimizer = optimizer);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "evfKTVK6S6hC"
      },
      "outputs": [],
      "source": [
        "z = Input(shape = (100,))\n",
        "img = generator(z)\n",
        "discriminator.trainable = False\n",
        "score = discriminator(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1o3mD-mYTkcr"
      },
      "outputs": [],
      "source": [
        "combined = Model(z, score)\n",
        "combined.compile(loss = 'binary_crossentropy', optimizer = optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "pkAsBZmjTsf5"
      },
      "outputs": [],
      "source": [
        "# with tf.device('/device:GPU:0'):\n",
        "# train(epochs = 25000, batch_size = 64, save_interval = 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ToWRu7NuFmZ1"
      },
      "outputs": [],
      "source": [
        "# generator.save_weights(\"/content/drive/MyDrive/Digits/Saved Model/model_weights.h5\")\n",
        "# discriminator.save_weights(\"/content/drive/MyDrive/Digits/Saved Model/discriminator_weights.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "8fVfzBFuUmAY"
      },
      "outputs": [],
      "source": [
        "# Save the entire model as a h5 file\n",
        "# generator.save(\"/content/drive/MyDrive/Digits/Saved Model/generator.h5\")\n",
        "# discriminator.save(\"/content/drive/MyDrive/Digits/Saved Model/discriminator.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ltVpov36arpS"
      },
      "outputs": [],
      "source": [
        "def is_fake(discriminator, image):\n",
        "    score = discriminator.predict(image)[0][0]\n",
        "    score = np.round(score, 0)\n",
        "    if score == 0:\n",
        "        print(\"Discriminator caught the FAKE image :(\")\n",
        "    else:\n",
        "        print(\"Discriminator suggests this is REAL, the GAN tricked it!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "PzyeJHJjVAca",
        "outputId": "a1d3492c-65bd-4eac-ca73-2089a9c04f77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "Discriminator suggests this is REAL, the GAN tricked it!\n"
          ]
        }
      ],
      "source": [
        "test_input = np.random.normal(0, 1, (1, 100))\n",
        "\n",
        "reconstructed_model = tf.keras.models.load_model(\"generator.h5\")\n",
        "gen_image = (reconstructed_model.predict(test_input))\n",
        "\n",
        "is_fake(discriminator, gen_image)\n",
        "\n",
        "# plt.imshow(gen_image.reshape((28, 28)), cmap='binary')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pX4QpnUI3Et",
        "outputId": "b106b71a-c7ae-4e0a-e4ee-2c52a8492933"
      },
      "outputs": [],
      "source": [
        "# gen_model = tf.keras.models.load_model(\"/content/drive/MyDrive/Digits/Saved Model/generator.h5\")\n",
        "# dis_model = tf.keras.models.load_model(\"/content/drive/MyDrive/Digits/Saved Model/discriminator.h5\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "kKMbEiXica51"
      },
      "outputs": [],
      "source": [
        "def generate_images_from_pretrained_model(model):\n",
        "    test_input = np.random.normal(0, 1, (1, 100))\n",
        "\n",
        "    gen_image = (model.predict(test_input))\n",
        "    # plt.imshow(gen_image.reshape((28, 28)), cmap='binary')\n",
        "    # return gen_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "OVxYgz4puq16",
        "outputId": "36bd6260-985f-417c-cad3-b0b5d60e23ef"
      },
      "outputs": [],
      "source": [
        "image = generate_images_from_pretrained_model(reconstructed_model)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Generative Adversarial Network for MNIST.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
