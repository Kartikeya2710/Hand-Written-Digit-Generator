{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Credits: DigitalSreeni"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vlCGBfBEF_o2"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Input, Dense, Reshape, Flatten\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "# import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsF869K6GvDQ",
        "outputId": "6a9b7ed4-c53e-4b2b-94de-3bfbd2520b1a"
      },
      "outputs": [],
      "source": [
        "# tf.__version__\n",
        "# device_name = tf.test.gpu_device_name()\n",
        "# if device_name != '/device:GPU:0':\n",
        "#   raise SystemError('GPU device not found')\n",
        "# print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Bwq4ANzaGTw2"
      },
      "outputs": [],
      "source": [
        "# The x tuple contains the images and the y tuple contains the ground truths of these images\n",
        "x, y = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WmVAlzZcGdSY"
      },
      "outputs": [],
      "source": [
        "#Define input image dimensions\n",
        "#Large images take too much time and resources.\n",
        "img_rows = 28\n",
        "img_cols = 28\n",
        "channels = 1\n",
        "img_shape = (img_rows, img_cols, channels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zbiXu0g2Gr3N"
      },
      "outputs": [],
      "source": [
        "#Given input of noise (latent) vector, the Generator produces an image.\n",
        "def build_generator():\n",
        "\n",
        "    noise_shape = (100,) #1D array of size 100 (latent vector / noise)\n",
        "\n",
        "    #Define your generator network \n",
        "    #Here we are only using Dense layers. But network can be complicated based\n",
        "    #on the application. For example, you can use VGG for super res. GAN.         \n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(256, input_shape=noise_shape))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(512))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(1024))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    \n",
        "    # Can use sigmoid maybe\n",
        "    model.add(Dense(np.prod(img_shape), activation='tanh'))\n",
        "    model.add(Reshape(img_shape))\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    noise = Input(shape=noise_shape)\n",
        "    generated_img = model(noise)    #Generated image\n",
        "\n",
        "    return Model(noise, generated_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7-p92ybiKCBE"
      },
      "outputs": [],
      "source": [
        "#Given an input image, the Discriminator outputs the likelihood of the image being real.\n",
        "#Binary classification - true or false (we're calling it score)\n",
        "\n",
        "def build_discriminator():\n",
        "\n",
        "    model = Sequential([\n",
        "        Flatten(input_shape=img_shape),\n",
        "        Dense(512),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "        Dense(256),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.summary()\n",
        "    img = Input(shape=img_shape)\n",
        "    score = model(img)\n",
        "    return Model(img, score)\n",
        "\n",
        "#The score is the Discriminator’s guess of input being real or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZJse7ttVLBpJ"
      },
      "outputs": [],
      "source": [
        "#Now that we have constructed our two models it’s time to pit them against each other.\n",
        "#We do this by defining a training function, loading the data set, re-scaling our training\n",
        "#images and setting the ground truths.\n",
        "\n",
        "def train(epochs, batch_size=128, save_interval=100):\n",
        "\n",
        "    # Load the (real images) dataset\n",
        "    (X_train, _), (_, _) = mnist.load_data()\n",
        "\n",
        "    # Convert to float and Rescale -1 to 1 (Can also do 0 to 1)\n",
        "    X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
        "\n",
        "    #Add channels dimension. As the input to our gen and discr. has a shape 28x28x1.\n",
        "    X_train = np.expand_dims(X_train, axis=3) \n",
        "\n",
        "    # Define the number of fake images that will be fed to the discriminator at a time\n",
        "    half_batch = int(batch_size / 2)\n",
        "\n",
        "    #We then loop through a number of epochs to train our Discriminator by first selecting\n",
        "    #a random batch of images from our true dataset, generating a set of images from our\n",
        "    #Generator, feeding both set of images into our Discriminator, and finally setting the\n",
        "    #loss parameters for both the real and fake images, as well as the combined loss. \n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "        # Select a random half batch of real images from the 60,000 images in the dataset\n",
        "        idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
        "        imgs = X_train[idx]\n",
        "        \n",
        "        # Generate normal noise / latent vector\n",
        "        noise = np.random.normal(0, 1, (half_batch, 100))\n",
        "\n",
        "        # Generate a half batch of fake images\n",
        "        gen_imgs = generator.predict(noise)\n",
        "\n",
        "        # Train the discriminator on real and fake images, separately\n",
        "        # Research showed that separate training is more effective. \n",
        "        # The np.ones and np.zeros show the class of the real and fake images respectively\n",
        "        d_loss_real = discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\n",
        "        d_loss_fake = discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n",
        "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "        # And within the same loop we train our Generator, by setting the input noise and\n",
        "        # ultimately training the Generator to have the Discriminator label its samples as valid\n",
        "        # by specifying the gradient loss.\n",
        "        # ---------------------\n",
        "        #  Train Generator\n",
        "        # ---------------------\n",
        "        #Create noise vectors as input for generator. \n",
        "        #Create as many noise vectors as defined by the batch size. \n",
        "        #Based on normal distribution. Output will be of size (batch size, 100)\n",
        "        noise = np.random.normal(0, 1, (batch_size, 100))\n",
        "\n",
        "        # The generator wants the discriminator to label the generated samples\n",
        "        # as valid (ones)\n",
        "        #This is where the generator is trying to trick discriminator into believing\n",
        "        #the generated image is true (hence value of y is 1)\n",
        "        valid_y = np.array([1] * batch_size)\n",
        "\n",
        "        # Generator is part of combined where it got directly linked with the discriminator\n",
        "        # Train the generator with noise as x and y as 1. \n",
        "        # Again, 1 as the output as it is adversarial and if generator did a great\n",
        "        #job of fooling the discriminator then the output would be 1 (true)\n",
        "        g_loss = combined.train_on_batch(noise, valid_y)\n",
        "\n",
        "\n",
        "        print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
        "\n",
        "        # If at save interval => save generated image samples\n",
        "        if epoch % save_interval == 0:\n",
        "            save_imgs(epoch)\n",
        "\n",
        "        #when the specific sample_interval is hit, we call the\n",
        "        #sample_image function. Which looks as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zsrP6M_yN06T"
      },
      "outputs": [],
      "source": [
        "def save_imgs(epoch):\n",
        "    r, c = 5, 5\n",
        "    noise = np.random.normal(0, 1, (r * c, 100))\n",
        "    gen_imgs = generator.predict(noise)\n",
        "\n",
        "    # Rescale images 0 - 1\n",
        "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "    fig, axs = plt.subplots(r, c)\n",
        "    cnt = 0\n",
        "    for i in range(r):\n",
        "        for j in range(c):\n",
        "            axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
        "            axs[i,j].axis('off')\n",
        "            cnt += 1\n",
        "    fig.savefig(\"/content/drive/MyDrive/Digits/Generated Images/mnist_%d.png\" % epoch)\n",
        "    plt.close()\n",
        "# This function saves our images for us to view"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Cb1QMAQrRnLm"
      },
      "outputs": [],
      "source": [
        "#Let us also define our optimizer for easy use later on.\n",
        "#That way if you change your mind, you can change it easily here\n",
        "optimizer = Adam(0.0002, 0.5)  #Learning rate and momentum."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQeS-IpaRyQK",
        "outputId": "1b967f16-4834-4886-a2d7-257bf1d6e68d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 533,505\n",
            "Trainable params: 533,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "discriminator = build_discriminator();\n",
        "discriminator.compile(loss='binary_crossentropy',\n",
        "                      optimizer = optimizer,\n",
        "                      metrics = ['accuracy']);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haLv1gbiST3K",
        "outputId": "9fb9f5dc-1a56-4b17-ca49-c370674975b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 256)               25856     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1024)              525312    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 784)               803600    \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 28, 28, 1)         0         \n",
            "=================================================================\n",
            "Total params: 1,493,520\n",
            "Trainable params: 1,489,936\n",
            "Non-trainable params: 3,584\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "generator = build_generator();\n",
        "generator.compile(loss = 'binary_crossentropy', optimizer = optimizer);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "evfKTVK6S6hC"
      },
      "outputs": [],
      "source": [
        "z = Input(shape = (100,))\n",
        "img = generator(z)\n",
        "discriminator.trainable = False\n",
        "score = discriminator(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "1o3mD-mYTkcr"
      },
      "outputs": [],
      "source": [
        "combined = Model(z, score)\n",
        "combined.compile(loss = 'binary_crossentropy', optimizer = optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkAsBZmjTsf5",
        "outputId": "1ae13f83-b089-462e-9667-da38f6d24cfe"
      },
      "outputs": [],
      "source": [
        "# with tf.device('/device:GPU:0'):\n",
        "# train(epochs = 50000, batch_size = 32, save_interval = 1000)\n",
        "# generator.save(\"/content/drive/MyDrive/Digits/Saved Model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ltVpov36arpS"
      },
      "outputs": [],
      "source": [
        "# To be used only after training the model or after saving the discriminator\n",
        "\n",
        "def is_fake(discriminator, image):\n",
        "    score = discriminator.predict(image)[0][0]\n",
        "    score = np.round(score, 0)\n",
        "    if score == 0:\n",
        "        print(\"Discriminator caught the FAKE image :(\")\n",
        "    else:\n",
        "        print(\"Discriminator suggests this is REAL, the GAN tricked it!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loading the saved model\n",
        "reconstructed_model = tf.keras.models.load_model(\"Saved Model\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "kKMbEiXica51"
      },
      "outputs": [],
      "source": [
        "def generate_images_from_pretrained_model(model):\n",
        "    test_input = np.random.normal(0, 1, (1, 100))\n",
        "\n",
        "    gen_image = (model.predict(test_input))\n",
        "    # plt.imshow(gen_image.reshape((28, 28)), cmap='binary')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "OG8vNfe6c40f",
        "outputId": "2fb7a3dc-c5d4-46ed-85d5-79f695b7ae11"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "generate_images_from_pretrained_model(reconstructed_model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Generative Adversarial Network for MNIST.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
